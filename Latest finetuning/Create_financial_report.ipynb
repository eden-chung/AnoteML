{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqBqSbB-fgIF",
        "outputId": "b2c6164e-890e-49dd-bf37-400398ae7970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsAiIHaK0zWz",
        "outputId": "28fc13eb-ea14-4230-f1af-a6af7b959fae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.352-py3-none-any.whl (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.4/794.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.6-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.3-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain)\n",
            "  Downloading langsmith-0.0.75-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.352 langchain-community-0.0.6 langchain-core-0.1.3 langsmith-0.0.75 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ratelimit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUe8cGnx1F1Q",
        "outputId": "995b21c8-89cf-4d2a-8fee-a61fe9042a91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ratelimit\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ratelimit\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5894 sha256=d0edae903e9dc738e50736602c16ba4c2e0bb7c4f5ce0acbfc72fefab92677d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/5f/ba/e972a56dcbf5de9f2b7d2b2a710113970bd173c4dcd3d2c902\n",
            "Successfully built ratelimit\n",
            "Installing collected packages: ratelimit\n",
            "Successfully installed ratelimit-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "YwBa8sz81Xfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "uO0PsVer2XUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain.vectorstores import Chroma, Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import datetime\n",
        "import shutil\n",
        "from openai import OpenAI\n",
        "import ratelimit\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "WD_CUYykgAXc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Module for fetching data from the SEC EDGAR Archives\"\"\"\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import sys\n",
        "\n",
        "if sys.version_info < (3, 8):\n",
        "    from typing_extensions import Final\n",
        "else:\n",
        "    from typing import Final\n",
        "\n",
        "import webbrowser\n",
        "\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "\n",
        "VALID_FILING_TYPES: Final[List[str]] = [\n",
        "    \"10-K\",\n",
        "    \"10-Q\",\n",
        "    \"S-1\",\n",
        "    \"10-K/A\",\n",
        "    \"10-Q/A\",\n",
        "    \"S-1/A\",\n",
        "]\n",
        "\n",
        "SEC_ARCHIVE_URL: Final[str] = \"https://www.sec.gov/Archives/edgar/data\"\n",
        "SEC_SEARCH_URL: Final[str] = \"http://www.sec.gov/cgi-bin/browse-edgar\"\n",
        "SEC_SUBMISSIONS_URL = \"https://data.sec.gov/submissions\"\n",
        "\n",
        "\n",
        "def get_filing(\n",
        "    cik: Union[str, int], accession_number: Union[str, int], company: str, email: str\n",
        ") -> str:\n",
        "    \"\"\"Fetches the specified filing from the SEC EDGAR Archives. Conforms to the rate\n",
        "    limits specified on the SEC website.\n",
        "    ref: https://www.sec.gov/os/accessing-edgar-data\"\"\"\n",
        "    session = _get_session(company, email)\n",
        "    return _get_filing(session, cik, accession_number)\n",
        "\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=10, period=1)\n",
        "def _get_filing(\n",
        "    session: requests.Session, cik: Union[str, int], accession_number: Union[str, int]\n",
        ") -> str:\n",
        "    \"\"\"Wrapped so filings can be retrieved with an existing session.\"\"\"\n",
        "    url = archive_url(cik, accession_number)\n",
        "    response = session.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=10, period=1)\n",
        "def get_cik_by_ticker(session: requests.Session, ticker: str) -> str:\n",
        "    \"\"\"Gets a CIK number from a stock ticker by running a search on the SEC website.\"\"\"\n",
        "    cik_re = re.compile(r\".*CIK=(\\d{10}).*\")\n",
        "    url = _search_url(ticker)\n",
        "    response = session.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    results = cik_re.findall(response.text)\n",
        "    return str(results[0])\n",
        "\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=10, period=1)\n",
        "def get_forms_by_cik(session: requests.Session, cik: Union[str, int]) -> dict:\n",
        "    \"\"\"Gets retrieves dict of recent SEC form filings for a given cik number.\"\"\"\n",
        "    json_name = f\"CIK{cik}.json\"\n",
        "    response = session.get(f\"{SEC_SUBMISSIONS_URL}/{json_name}\")\n",
        "    response.raise_for_status()\n",
        "    content = json.loads(response.content)\n",
        "    recent_forms = content[\"filings\"][\"recent\"]\n",
        "    form_types = {k: v for k, v in zip(recent_forms[\"accessionNumber\"], recent_forms[\"form\"])}\n",
        "    return form_types\n",
        "\n",
        "\n",
        "def _get_recent_acc_num_by_cik(\n",
        "    session: requests.Session, cik: Union[str, int], form_types: List[str]\n",
        ") -> Tuple[str, str]:\n",
        "    \"\"\"Returns accession number and form type for the most recent filing for one of the\n",
        "    given form_types (AKA filing types) for a given cik.\"\"\"\n",
        "    retrieved_form_types = get_forms_by_cik(session, cik)\n",
        "    for acc_num, form_type_ in retrieved_form_types.items():\n",
        "        if form_type_ in form_types:\n",
        "            return _drop_dashes(acc_num), form_type_\n",
        "    raise ValueError(f\"No filings found for {cik}, looking for any of: {form_types}\")\n",
        "\n",
        "\n",
        "def get_recent_acc_by_cik(\n",
        "    cik: str,\n",
        "    form_type: str,\n",
        "    company: Optional[str] = None,\n",
        "    email: Optional[str] = None,\n",
        ") -> Tuple[str, str]:\n",
        "    \"\"\"Returns (accession_number, retrieved_form_type) for the given cik and form_type.\n",
        "    The retrieved_form_type may be an amended version of requested form_type, e.g. 10-Q/A for 10-Q.\n",
        "    \"\"\"\n",
        "    session = _get_session(company, email)\n",
        "    return _get_recent_acc_num_by_cik(session, cik, _form_types(form_type))\n",
        "\n",
        "\n",
        "def get_recent_cik_and_acc_by_ticker(\n",
        "    ticker: str,\n",
        "    form_type: str,\n",
        "    company: Optional[str] = None,\n",
        "    email: Optional[str] = None,\n",
        ") -> Tuple[str, str, str]:\n",
        "    \"\"\"Returns (cik, accession_number, retrieved_form_type) for the given ticker and form_type.\n",
        "    The retrieved_form_type may be an amended version of requested form_type, e.g. 10-Q/A for 10-Q.\n",
        "    \"\"\"\n",
        "    session = _get_session(company, email)\n",
        "    cik = get_cik_by_ticker(session, ticker)\n",
        "    acc_num, retrieved_form_type = _get_recent_acc_num_by_cik(session, cik, _form_types(form_type))\n",
        "    return cik, acc_num, retrieved_form_type\n",
        "\n",
        "\n",
        "def get_form_by_ticker(\n",
        "    ticker: str,\n",
        "    form_type: str,\n",
        "    allow_amended_filing: Optional[bool] = True,\n",
        "    company: Optional[str] = None,\n",
        "    email: Optional[str] = None,\n",
        ") -> str:\n",
        "    \"\"\"For a given ticker, gets the most recent form of a given form_type.\"\"\"\n",
        "    session = _get_session(company, email)\n",
        "    cik = get_cik_by_ticker(session, ticker)\n",
        "    return get_form_by_cik(\n",
        "        cik, form_type, allow_amended_filing=allow_amended_filing, company=company, email=email\n",
        "    )\n",
        "\n",
        "\n",
        "def _form_types(form_type: str, allow_amended_filing: Optional[bool] = True):\n",
        "    \"\"\"Potentialy expand to include amended filing, e.g.:\n",
        "    \"10-Q\" -> \"10-Q/A\"\n",
        "    \"\"\"\n",
        "    assert form_type in VALID_FILING_TYPES\n",
        "    if allow_amended_filing and not form_type.endswith(\"/A\"):\n",
        "        return [form_type, f\"{form_type}/A\"]\n",
        "    else:\n",
        "        return [form_type]\n",
        "\n",
        "\n",
        "def get_form_by_cik(\n",
        "    cik: str,\n",
        "    form_type: str,\n",
        "    allow_amended_filing: Optional[bool] = True,\n",
        "    company: Optional[str] = None,\n",
        "    email: Optional[str] = None,\n",
        ") -> str:\n",
        "    \"\"\"For a given CIK, returns the most recent form of a given form_type. By default\n",
        "    an amended version of the form_type may be retrieved (allow_amended_filing=True).\n",
        "    E.g., if form_type is \"10-Q\", the retrived form could be a 10-Q or 10-Q/A.\n",
        "    \"\"\"\n",
        "    session = _get_session(company, email)\n",
        "    acc_num, _ = _get_recent_acc_num_by_cik(\n",
        "        session, cik, _form_types(form_type, allow_amended_filing)\n",
        "    )\n",
        "    text = _get_filing(session, cik, acc_num)\n",
        "    return text\n",
        "\n",
        "\n",
        "def open_form(cik, acc_num):\n",
        "    \"\"\"For a given cik and accession number, opens the index page in default browser for the\n",
        "    associated SEC form\"\"\"\n",
        "    acc_num = _drop_dashes(acc_num)\n",
        "    webbrowser.open_new_tab(f\"{SEC_ARCHIVE_URL}/{cik}/{acc_num}/{_add_dashes(acc_num)}-index.html\")\n",
        "\n",
        "\n",
        "def open_form_by_ticker(\n",
        "    ticker: str,\n",
        "    form_type: str,\n",
        "    allow_amended_filing: Optional[bool] = True,\n",
        "    company: Optional[str] = None,\n",
        "    email: Optional[str] = None,\n",
        "):\n",
        "    \"\"\"For a given ticker, opens the index page in default browser for the most recent form of a\n",
        "    given form_type.\"\"\"\n",
        "    session = _get_session(company, email)\n",
        "    cik = get_cik_by_ticker(session, ticker)\n",
        "    acc_num, _ = _get_recent_acc_num_by_cik(\n",
        "        session, cik, _form_types(form_type, allow_amended_filing)\n",
        "    )\n",
        "    open_form(cik, acc_num)\n",
        "\n",
        "\n",
        "def archive_url(cik: Union[str, int], accession_number: Union[str, int]) -> str:\n",
        "    \"\"\"Builds the archive URL for the SEC accession number. Looks for the .txt file for the\n",
        "    filing, while follows a {accession_number}.txt format.\"\"\"\n",
        "    filename = f\"{_add_dashes(accession_number)}.txt\"\n",
        "    accession_number = _drop_dashes(accession_number)\n",
        "    return f\"{SEC_ARCHIVE_URL}/{cik}/{accession_number}/{filename}\"\n",
        "\n",
        "\n",
        "def _search_url(cik: Union[str, int]) -> str:\n",
        "    search_string = f\"CIK={cik}&Find=Search&owner=exclude&action=getcompany\"\n",
        "    url = f\"{SEC_SEARCH_URL}?{search_string}\"\n",
        "    return url\n",
        "\n",
        "\n",
        "def _add_dashes(accession_number: Union[str, int]) -> str:\n",
        "    \"\"\"Adds the dashes back into the accession number\"\"\"\n",
        "    accession_number = str(accession_number)\n",
        "    return f\"{accession_number[:10]}-{accession_number[10:12]}-{accession_number[12:]}\"\n",
        "\n",
        "\n",
        "def _drop_dashes(accession_number: Union[str, int]) -> str:\n",
        "    \"\"\"Converts the accession number to the no dash representation.\"\"\"\n",
        "    accession_number = str(accession_number).replace(\"-\", \"\")\n",
        "    return accession_number.zfill(18)\n",
        "\n",
        "\n",
        "def _get_session(company: Optional[str] = None, email: Optional[str] = None) -> requests.Session:\n",
        "    \"\"\"Creates a requests sessions with the appropriate headers set. If these headers are not\n",
        "    set, SEC will reject your request.\n",
        "    ref: https://www.sec.gov/os/accessing-edgar-data\"\"\"\n",
        "    if company is None:\n",
        "        company = os.environ.get(\"SEC_API_ORGANIZATION\")\n",
        "    if email is None:\n",
        "        email = os.environ.get(\"SEC_API_EMAIL\")\n",
        "    assert company\n",
        "    assert email\n",
        "    session = requests.Session()\n",
        "    session.headers.update(\n",
        "        {\n",
        "            \"User-Agent\": f\"{company} {email}\",\n",
        "            \"Content-Type\": \"text/html\",\n",
        "        }\n",
        "    )\n",
        "    return session\n"
      ],
      "metadata": {
        "id": "1bWsyDmOgaHh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from typing import List, Optional, Tuple, Union\n",
        "import sys"
      ],
      "metadata": {
        "id": "LScvJAiBgbF4"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = get_form_by_ticker(\n",
        "    'aapl',\n",
        "    '10-K',\n",
        "    company='Unstructured Technologies',\n",
        "    email='support@unstructured.io'\n",
        ")"
      ],
      "metadata": {
        "id": "9ijzUc5CgcUz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "OPENAI_API_KEY = \"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\"\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key= OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "_QnMiKirgdFG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_embed_xml(text):\n",
        "    \"\"\"Clean the text from the XML file\"\"\"\n",
        "\n",
        "    cleaned_text = re.sub('<[^>]+>', '', text)\n",
        "\n",
        "    return cleaned_text\n"
      ],
      "metadata": {
        "id": "g1kaP-BIgeGW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_knowledge_hub(plaintext):\n",
        "    \"\"\"From a 10-K document, create a Chroma DB knowledge hub.\n",
        "\n",
        "    Args:\n",
        "        path_to_10k: Relative path to the 10-K hosted locally on the user's computer\n",
        "\n",
        "    Returns:\n",
        "        vectordb: The vector database with the information from the 10-K\n",
        "        db_directory: The path to the vector database\n",
        "    \"\"\"\n",
        "\n",
        "    now = datetime.datetime.now()\n",
        "    timestamp = now.strftime(\"%Y%m%d%H%M%S\")\n",
        "    db_directory = \"db_\" + timestamp\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1200,\n",
        "        chunk_overlap=5,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "        length_function=len)\n",
        "\n",
        "    documents_doc = [Document(page_content=plaintext)]\n",
        "\n",
        "    texts = splitter.split_documents(documents_doc)\n",
        "\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=texts,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=db_directory\n",
        "    )\n",
        "    vectordb.persist()\n",
        "\n",
        "    return vectordb, db_directory"
      ],
      "metadata": {
        "id": "6RKgxPF0guSj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_chroma_db(db_directory):\n",
        "    \"\"\"Deletes the Chroma DB created locally on the computer\n",
        "\n",
        "    Args:\n",
        "        db_directory: The path to the vector database\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        shutil.rmtree(db_directory)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Chroma database '{db_directory}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting Chroma database: {str(e)}\")"
      ],
      "metadata": {
        "id": "YsCAjgXMgwvd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_gpt_finetuned_model(ticker, question):\n",
        "    \"\"\"Utilizes the SEC's EDGAR API to ask the fine-tuned GPT model a question based off a ticker.\n",
        "\n",
        "    Args:\n",
        "        ticker: A company's ticker (eg AAPL)\n",
        "        question: Question to ask the model\n",
        "\n",
        "    Returns:\n",
        "        answer: The answer given by the fine-tuned GPT model\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        text = get_form_by_ticker(ticker, '10-K', company='Unstructured Technologies', email='support@unstructured.io')\n",
        "    except Exception as e:\n",
        "        print(f\"Error. This ticker is not valid. Please input a valid ticker\")\n",
        "        return\n",
        "\n",
        "    text = process_and_embed_xml(text)\n",
        "\n",
        "    db, db_dir = create_knowledge_hub(text)\n",
        "\n",
        "    source1 = db.similarity_search(question, k = 2)[0].page_content\n",
        "    source2 = db.similarity_search(question, k = 2)[1].page_content\n",
        "\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0613:personal:anote:8DO8V2LB\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a factual chatbot that answers questions about 10-K documents. You only answer with answers you find in the text, no outside information.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{source1}{source2} Now, this is our question: {question}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    delete_chroma_db(db_dir)\n",
        "\n",
        "    answer = completion.choices[0].message.content\n",
        "\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "tzmnPOWvgxwJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_gpt_finetuned_model(\"aapl\", \"What are the company's strategic priorities and growth prospects?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "f8ngKe6xgy0p",
        "outputId": "7e70a0ba-ef23-4546-fd9d-d2e97094037c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The company's strategic priorities are to expand its market presence in China, expand existing markets, stay ahead of demand in mobile product markets and provide superior customer experience through hardware, software and services. The company's growth prospects include expanding the app store and third-party services and support.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlCe2AxWg0_i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL2liNIK3ZzM",
        "outputId": "73d387b7-508f-4755-d460-a70c5d063ea8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40703 sha256=770447a02fab86b79af86dc8a425614723e6579c2fa1190d9f20e26f74a7154a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#INPUT YOUR TICKER\n",
        "ticker = \"aapl\"\n",
        "\n",
        "#INPUT SAMPLE QUESTIONS\n",
        "sample_questions = [\"What are the company's strategic priorities and growth prospects?\", \"What is the company's revenue in 2022?\", \"How does the company generate its revenue? What are its main products or services?\", \"What are the major drivers of revenue and profit growth or decline?\"]"
      ],
      "metadata": {
        "id": "SlfmEgq-7Vel"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font('Times', 'B', 16)\n",
        "#pdf.cell(40, 10, f'Financial Report on company: {ticker}')\n",
        "pdf.write(4, f'Financial Report on company: {ticker.upper()}\\n')\n",
        "\n",
        "\n",
        "for question in sample_questions:\n",
        "  answer = ask_gpt_finetuned_model(ticker, question)\n",
        "  #answer = \"Apple's strategic priorities are primarily directed towards the expansion of product sales, improvements to existing products and services, and entering into new markets through acquisitions and joint ventures\"\n",
        "  answer_encoded = answer.encode('latin-1', 'replace').decode('latin-1')\n",
        "  question_encoded = question.encode('latin-1', 'replace').decode('latin-1')\n",
        "\n",
        "  pdf.ln(h=10)\n",
        "  pdf.set_font('Times', 'B', 12)\n",
        "  pdf.write(1, question_encoded)\n",
        "  pdf.set_font('Times', '', 12)\n",
        "  pdf.ln(h=5)\n",
        "  pdf.multi_cell(0, 5, answer_encoded)\n",
        "\n",
        "\n",
        "pdf.output('financial_report.pdf', 'F')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lH6ykFUK55_B",
        "outputId": "77889f02-a206-43ac-c3cb-27a1053b60d3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YV5KUOro6zY2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}